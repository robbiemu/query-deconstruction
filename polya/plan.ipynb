{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    #model = \"mistral-small:22b-instruct-2409-q6_K\",\n",
    "    model = \"qwen2.5:32b-instruct-q6_K\",\n",
    "    #model = \"finalend/hermes-3-llama-3.1:70b-q3_K_M\",\n",
    "    temperature = 0,\n",
    "    num_ctx=8192,\n",
    "    num_predict = 8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"Five people (A, B, C, D, and E) are in a room. A is watching TV with B, D is sleeping, B is eating chow min, and E is playing table tennis. Suddenly, a call comes on the telephone. B goes out of the room to pick up the call. What is C doing?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from understanding import ProblemUnderstanding\n",
    "from langchain_core.messages import HumanMessage\n",
    "from models import State, Terms\n",
    "\n",
    "import yaml\n",
    "\n",
    "def human_feedback(terms: Terms) -> Terms:\n",
    "    updated_terms = terms.copy()\n",
    "    term_definitions = dict()\n",
    "    for term in terms[\"doubts\"]:\n",
    "        if not (term in updated_terms[\"term_definitions\"]):\n",
    "            definition = input(f\"TERM '{term}': \")\n",
    "            term_definitions[term] = definition\n",
    "    \n",
    "    return {\"term_definitions\": term_definitions}\n",
    "\n",
    "initial_state = State(messages=[HumanMessage(content=problem)])\n",
    "\n",
    "problem_understanding = ProblemUnderstanding(\"simple_prompts\", llm, human_feedback)\n",
    "interim = problem_understanding.understand_problem(initial_state, terms_recursion_limit=1, recursion_limit=2)\n",
    "\n",
    "print(yaml.dump(interim[\"understanding\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_preparation = PlanPreparation(\"prompts\", llm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
